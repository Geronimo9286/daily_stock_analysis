# -*- coding: utf-8 -*-
"""
===================================
Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä¸»è°ƒåº¦ç¨‹åº (å¢å¼ºç‰ˆ)
===================================

ä¿®æ”¹è¯´æ˜ï¼š
1. æ–°å¢è‡ªåŠ¨æŠ“å–å½“æ—¥æ¶¨å¹…æ¦œå‰10åä¸ªè‚¡çš„åŠŸèƒ½ã€‚
2. åŠ¨æ€åˆå¹¶ç¯å¢ƒå˜é‡ STOCK_LIST ä¸çƒ­é—¨ä¸ªè‚¡ï¼Œå»é‡åç»Ÿä¸€åˆ†æã€‚
"""
import os

# ä»£ç†é…ç½® - ä»…åœ¨æœ¬åœ°ç¯å¢ƒä½¿ç”¨ï¼ŒGitHub Actions ä¸éœ€è¦
if os.getenv("GITHUB_ACTIONS") != "true":
    pass

import argparse
import logging
import sys
import time
import akshare as ak  # ç¡®ä¿å¯¼å…¥ akshare ç”¨äºè·å–çƒ­é—¨è‚¡
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date, timezone, timedelta
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from feishu_doc import FeishuDocManager

from config import get_config, Config
from storage import get_db, DatabaseManager
from data_provider import DataFetcherManager
from data_provider.akshare_fetcher import AkshareFetcher, RealtimeQuote, ChipDistribution
from analyzer import GeminiAnalyzer, AnalysisResult, STOCK_NAME_MAP
from notification import NotificationService, NotificationChannel, send_daily_report
from search_service import SearchService, SearchResponse
from stock_analyzer import StockTrendAnalyzer, TrendAnalysisResult
from market_analyzer import MarketAnalyzer

# é…ç½®æ—¥å¿—æ ¼å¼
LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s'
LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'


def setup_logging(debug: bool = False, log_dir: str = "./logs") -> None:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = logging.DEBUG if debug else logging.INFO
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    today_str = datetime.now().strftime('%Y%m%d')
    log_file = log_path / f"stock_analysis_{today_str}.log"
    debug_log_file = log_path / f"stock_analysis_debug_{today_str}.log"
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(console_handler)
    file_handler = RotatingFileHandler(log_file, maxBytes=10 * 1024 * 1024, backupCount=5, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(file_handler)
    debug_handler = RotatingFileHandler(debug_log_file, maxBytes=50 * 1024 * 1024, backupCount=3, encoding='utf-8')
    debug_handler.setLevel(logging.DEBUG)
    debug_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(debug_handler)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('sqlalchemy').setLevel(logging.WARNING)
    logging.getLogger('google').setLevel(logging.WARNING)
    logging.getLogger('httpx').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


class StockAnalysisPipeline:
    def __init__(self, config: Optional[Config] = None, max_workers: Optional[int] = None):
        self.config = config or get_config()
        self.max_workers = max_workers or self.config.max_workers
        self.db = get_db()
        self.fetcher_manager = DataFetcherManager()
        self.akshare_fetcher = AkshareFetcher()
        self.trend_analyzer = StockTrendAnalyzer()
        self.analyzer = GeminiAnalyzer()
        self.notifier = NotificationService()
        self.search_service = SearchService(
            tavily_keys=self.config.tavily_api_keys,
            serpapi_keys=self.config.serpapi_keys,
        )
        logger.info(f"è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_workers}")

    def get_top_n_hot_stocks(self, n: int = 10) -> List[str]:
        """ä» AkShare è·å–å½“æ—¥æ¶¨å¹…å‰ N çš„ A è‚¡è‚¡ç¥¨ä»£ç """
        try:
            logger.info(f"æ­£åœ¨æŠ“å–å…¨ç›˜æ¶¨å¹…å‰ {n} çš„çƒ­é—¨è‚¡ç¥¨...")
            df = ak.stock_zh_a_spot_em()
            # è¿‡æ»¤æ‰ STã€é€€å¸‚è‚¡å’ŒåŒ—äº¤æ‰€(å¯é€‰)ï¼ŒæŒ‰æ¶¨å¹…æ’åº
            hot_df = df[~df['åç§°'].str.contains("ST|é€€å¸‚")].sort_values(by="æ¶¨è·Œå¹…", ascending=False)
            top_n = hot_df.head(n)
            
            code_list = []
            for _, row in top_n.iterrows():
                code = str(row['ä»£ç '])
                prefix = "sh" if code.startswith(('60', '68')) else "sz"
                code_list.append(f"{prefix}{code}")
            return code_list
        except Exception as e:
            logger.error(f"æŠ“å–çƒ­é—¨è‚¡ç¥¨å¤±è´¥: {e}")
            return []

    def fetch_and_save_stock_data(self, code: str, force_refresh: bool = False) -> Tuple[bool, Optional[str]]:
        try:
            today = date.today()
            if not force_refresh and self.db.has_today_data(code, today):
                return True, None
            df, source_name = self.fetcher_manager.get_daily_data(code, days=30)
            if df is None or df.empty: return False, "æ•°æ®ä¸ºç©º"
            self.db.save_daily_data(df, code, source_name)
            return True, None
        except Exception as e:
            return False, str(e)

    def analyze_stock(self, code: str) -> Optional[AnalysisResult]:
        # ... (æ­¤å¤„ä¿æŒä½ åŸæ¥ analyze_stock çš„é€»è¾‘ä¸å˜ï¼Œä¸ºèŠ‚çœç¯‡å¹…ç•¥å»é‡å¤ä»£ç )
        try:
            stock_name = STOCK_NAME_MAP.get(code, '')
            realtime_quote = None
            try:
                realtime_quote = self.akshare_fetcher.get_realtime_quote(code)
                if realtime_quote and realtime_quote.name: stock_name = realtime_quote.name
            except: pass
            if not stock_name: stock_name = f'è‚¡ç¥¨{code}'
            
            chip_data = None
            try: chip_data = self.akshare_fetcher.get_chip_distribution(code)
            except: pass
            
            trend_result = None
            try:
                context = self.db.get_analysis_context(code)
                if context and 'raw_data' in context:
                    import pandas as pd
                    df = pd.DataFrame(context['raw_data'])
                    trend_result = self.trend_analyzer.analyze(df, code)
            except: pass
            
            news_context = None
            if self.search_service.is_available:
                intel_results = self.search_service.search_comprehensive_intel(code, stock_name, max_searches=2)
                if intel_results: news_context = self.search_service.format_intel_report(intel_results, stock_name)
            
            context = self.db.get_analysis_context(code)
            if context is None: return None
            enhanced_context = self._enhance_context(context, realtime_quote, chip_data, trend_result, stock_name)
            return self.analyzer.analyze(enhanced_context, news_context=news_context)
        except Exception as e:
            logger.error(f"[{code}] åˆ†æå¤±è´¥: {e}")
            return None

    def _enhance_context(self, context, realtime_quote, chip_data, trend_result, stock_name):
        enhanced = context.copy()
        enhanced['stock_name'] = stock_name
        if realtime_quote:
            enhanced['realtime'] = {'price': realtime_quote.price, 'volume_ratio': realtime_quote.volume_ratio, 'turnover_rate': realtime_quote.turnover_rate}
        if chip_data:
            enhanced['chip'] = {'profit_ratio': chip_data.profit_ratio, 'concentration_90': chip_data.concentration_90}
        if trend_result:
            enhanced['trend_analysis'] = {'trend_status': trend_result.trend_status.value, 'buy_signal': trend_result.buy_signal.value, 'signal_score': trend_result.signal_score}
        return enhanced

    def process_single_stock(self, code: str, skip_analysis: bool = False) -> Optional[AnalysisResult]:
        try:
            self.fetch_and_save_stock_data(code)
            if skip_analysis: return None
            return self.analyze_stock(code)
        except Exception as e:
            logger.error(f"[{code}] å¼‚å¸¸: {e}")
            return None

    def run(self, stock_codes: Optional[List[str]] = None, dry_run: bool = False, send_notification: bool = True) -> List[AnalysisResult]:
        start_time = time.time()
        
        # --- æ ¸å¿ƒä¿®æ”¹é€»è¾‘å¼€å§‹ ---
        # 1. è·å–åŸºç¡€åˆ—è¡¨
        if stock_codes is None:
            stock_codes = self.config.stock_list
        
        # 2. åŠ¨æ€æŠ“å–å½“æ—¥æ¶¨å¹…æ¦œå‰10åªçƒ­é—¨è‚¡
        hot_stocks = self.get_top_n_hot_stocks(n=10)
        
        # 3. åˆå¹¶å¹¶å»é‡
        final_stock_list = list(dict.fromkeys(stock_codes + hot_stocks)) 
        # --- æ ¸å¿ƒä¿®æ”¹é€»è¾‘ç»“æŸ ---

        if not final_stock_list:
            logger.error("å¾…åˆ†æåˆ—è¡¨ä¸ºç©º")
            return []
        
        logger.info(f"===== åˆ†æå¼€å§‹ï¼Œæ€»è®¡ {len(final_stock_list)} åª (å«çƒ­é—¨è‚¡) =====")
        
        results: List[AnalysisResult] = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_code = {executor.submit(self.process_single_stock, code, skip_analysis=dry_run): code for code in final_stock_list}
            for future in as_completed(future_to_code):
                try:
                    res = future.result()
                    if res: results.append(res)
                except Exception as e:
                    logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        
        if results and send_notification and not dry_run:
            self._send_notifications(results)
        return results

    def _send_notifications(self, results: List[AnalysisResult]) -> None:
        try:
            report = self.notifier.generate_dashboard_report(results)
            self.notifier.save_report_to_file(report)
            if self.notifier.is_available():
                self.notifier.send(report)
        except Exception as e:
            logger.error(f"æ¨é€å¤±è´¥: {e}")

# ... (parse_arguments, run_market_review, run_full_analysis ç­‰åç»­å‡½æ•°ä¿æŒä¸å˜)

def run_market_review(notifier, analyzer=None, search_service=None) -> Optional[str]:
    logger.info("æ‰§è¡Œå¤§ç›˜å¤ç›˜...")
    try:
        market_analyzer = MarketAnalyzer(search_service=search_service, analyzer=analyzer)
        review_report = market_analyzer.run_daily_review()
        if review_report:
            notifier.send(f"ğŸ¯ å¤§ç›˜å¤ç›˜\n\n{review_report}")
            return review_report
    except Exception as e:
        logger.error(f"å¤§ç›˜å¤ç›˜å¤±è´¥: {e}")
    return None

def run_full_analysis(config, args, stock_codes):
    try:
        pipeline = StockAnalysisPipeline(config=config, max_workers=args.workers)
        # ä¸ªè‚¡åˆ†æ (å†…éƒ¨å·²åŒ…å«è‡ªåŠ¨æŠ“å–çƒ­é—¨è‚¡é€»è¾‘)
        results = pipeline.run(stock_codes=stock_codes, dry_run=args.dry_run, send_notification=not args.no_notify)
        
        # å¤§ç›˜å¤ç›˜
        market_report = ""
        if config.market_review_enabled and not args.no_market_review:
            market_report = run_market_review(pipeline.notifier, pipeline.analyzer, pipeline.search_service)

        # é£ä¹¦äº‘æ–‡æ¡£ç”Ÿæˆ
        try:
            feishu_doc = FeishuDocManager()
            if feishu_doc.is_configured() and (results or market_report):
                tz_cn = timezone(timedelta(hours=8))
                doc_title = f"{datetime.now(tz_cn).strftime('%Y-%m-%d %H:%M')} ç»¼åˆå¤ç›˜æŠ¥å‘Š"
                full_content = ""
                if market_report: full_content += f"# ğŸ“ˆ å¤§ç›˜å¤ç›˜\n\n{market_report}\n\n"
                if results: full_content += f"# ğŸš€ å†³ç­–ä»ªè¡¨ç›˜\n\n{pipeline.notifier.generate_dashboard_report(results)}"
                doc_url = feishu_doc.create_daily_doc(doc_title, full_content)
                if doc_url: pipeline.notifier.send(f"âœ… é£ä¹¦æ–‡æ¡£å·²ç”Ÿæˆ: {doc_url}")
        except Exception as e:
            logger.error(f"é£ä¹¦åŒæ­¥å¤±è´¥: {e}")
    except Exception as e:
        logger.exception(f"ä¸»æµç¨‹å¤±è´¥: {e}")

def main() -> int:
    args = parse_arguments()
    config = get_config()
    setup_logging(debug=args.debug, log_dir=config.log_dir)
    
    stock_codes = None
    if args.stocks:
        stock_codes = [code.strip() for code in args.stocks.split(',') if code.strip()]
    
    if args.market_review:
        run_market_review(NotificationService(), GeminiAnalyzer(api_key=config.gemini_api_key))
        return 0
    
    run_full_analysis(config, args, stock_codes)
    return 0

def parse_arguments():
    parser = argparse.ArgumentParser(description='Aè‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ')
    parser.add_argument('--debug', action='store_true')
    parser.add_argument('--dry-run', action='store_true')
    parser.add_argument('--stocks', type=str)
    parser.add_argument('--no-notify', action='store_true')
    parser.add_argument('--workers', type=int)
    parser.add_argument('--market-review', action='store_true')
    parser.add_argument('--no-market-review', action='store_true')
    parser.add_argument('--schedule', action='store_true')
    return parser.parse_args()

if __name__ == "__main__":
    sys.exit(main())
